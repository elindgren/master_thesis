\chapter{Method}
\label{chap:method}

\todo{Fixa tempus}
In this chapter we present our methodology for developing accurate models that predicts age and sex, and how they can be analysed in order to gain insight into which functional brain networks in the data are related to age and sex. The chapter can be summarised as follows: first, we present the preprocessing of the \acrshort{fmri} data into graphs suitable for graph neural networks. Secondly, the models developed for predicting sex and age based on graph and node prediction are presented, followed by methods for analysing and creating saliency maps from the models. See Appendix \ref{app:model_training} for more details on the exact model architectures used. \todo{Lägga här eller flytta?}

\section{From fMRI scans to graphs}\label{sec:fmri_to_graphs}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{chapters/images_methods/fmri_network_ukbiobank.png}
    \caption{A schematic visualisation of a network of functional brain networks. Blue and orange edges represent positive and negative correlation between brain networks, respectively.  The individual functional brain networks are represented as human brains with \acrshort{fmri} activations (orange), along the circumference of the circle. The node numbering is consistent with \cref{tab:Networks}. Image adapted from example at UK Biobank Brain Imaging Online Resources \cite{ukbiobank_brain_imaging}.}
    \label{fig:fmri_network}
\end{figure}

The data that makes up the foundation of this master thesis is from the UK Biobank \cite{ukbiobank}. In the UK Biobank, \acrshort{fmri} brain scans, which are the focus of this project, are available for roughly 35 000 subjects varying between 45 and 80 years of age. Functional \acrshort{mri} (\acrshort{fmri}) is a technique for measuring the neuronal activity in the human brain \cite{sporns}. In a measurement, the activations of various parts of the subject brain are measured as time series. The correlations between these time series can then be calculated and formed into networks. The regions that make up these networks do not necessarily have to be physically close together nor directly anatomically connected. The obtained \acrshort{fmri} brain networks are often associated with certain functionalities of the brain. One example of such a functional brain network is the \acrfull{dmn}, which handles memory processing and mind wandering  \cite{alves_dmn}. 

\todo{Byt färg på orange/blå i figuren om vi verkligen vill använda den här som atlas. Just nu okej, eftersom den bara är schematisk, men annars är den inkorrekt. }

\begin{table}[!htbp]
    \centering
    \caption{A key of which node corresponds to what functional brain network in the data from the UK Biobank. The node numbering is consistent with \cref{fig:fmri_network}. Each node will in the remainder of this thesis be referred to by the abbreviation for its functional brain network.}
    \begin{tabular}{||c|c|c||}
        \hline
        Number & Network & Abbreviation  \\ \hline\hline
        1 & \acrlong{cb1} & \acrshort{cb1} \\ \hline
        2 & \acrlong{vl} & \acrshort{vl}  \\ \hline
        3 & \acrlong{ssn} & \acrshort{ssn} \\ \hline
        4 & \acrlong{vm} &  \acrshort{vm}\\ \hline
        5 & \acrlong{dar} & \acrshort{dar} \\ \hline
        6 & \acrlong{dal} &  \acrshort{dal} \\ \hline
        7 & \acrlong{fp} &  \acrshort{fp}\\ \hline
        8 & \acrlong{vv1m} & \acrshort{vv1m} \\ \hline
        9 & \acrlong{dmn} &  \acrshort{dmn} \\ \hline
        10 & \acrlong{pss} & \acrshort{pss} \\ \hline
        11 & \acrlong{pmn} & \acrshort{pmn} \\ \hline
        12 & \acrlong{smm} & \acrshort{smm}  \\ \hline
        13 & \acrlong{va} &  \acrshort{va}\\ \hline
        14 & \acrlong{sn} &  \acrshort{sn}\\ \hline
        15 & \acrlong{cb2} & \acrshort{cb2} \\ \hline
        16 & \acrlong{pl} & \acrshort{pl} \\ \hline
        17 & \acrlong{ts} & \acrshort{ts} \\ \hline
        18 & \acrlong{bg} &  \acrshort{bg} \\ \hline
        19 & \acrlong{vv1l} & \acrshort{vv1l} \\ \hline
        20 & \acrlong{pmc} & \acrshort{pmc} \\ \hline
        21 & \acrlong{tm} & \acrshort{tm} \\ \hline
    \end{tabular}
    \label{tab:Networks}
\end{table}


\todo{iterera detta stycke}
The \acrshort{fmri} data available from the UK Biobank for this project has also been further preprocessed. The \acrshort{fmri} network consisting of the correlations between all brain regions have been clustered into 21 functional networks, and the functional networks are connected to form a \textit{network of networks}. This network of networks is represented as a graph of 21 nodes, where the weight of the edges between the nodes is calculated as the average correlation between the smaller (functional) networks that make up the nodes. Each node in the brain graph thus corresponds to one functional network in the human brain (such as the DMN) and the graph represents how the functional networks in the brain are correlated. For a list of what functional brain network each node corresponds to, and its abbreviation, see \cref{tab:Networks}. The weights in the brain graph can both be positive and negative, depending on the correlation. A visualization of such a brain graph is given in figure \ref{fig:fmri_network}, in which the various nodes and the functional brain networks they represent form the outer circle, with the coloured lines connecting the nodes being the edges in the graph. The lines are coloured according to the edge weights, with blue and orange lines having positive and negative weights respectively.



\subsection{Handling negative edge weights}

A brain graph with both positive and negative weights may however be problematic. Specifically, when normalizing the adjacency matrices according to equation \eqref{eq:renormalization_trick}, the risk of dividing by zero becomes imminent. There are several ways to handle this problem. One alternative is to only study all positive or all negative connections, and another might be to take the absolute value of all connections. We however decided to split the negative and positive connections into two separate graphs and thus form a multiplex graph. From a practical point of view this was implemented by creating a block diagonal adjacency matrix $A_{split}$ with all positive connections in the upper block and all negative connections in the lower block, see \cref{fig:block_diagonal_adjacency_matrix}. The connections in the positive block that were negative in the original adjacency matrix $A$ were replaced with zeros, and vice versa in the negative block.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{chapters/images_methods/split.png}
    \caption{An example of how negative values in an adjacency matrix $A$ are handled. The negative values (orange) are extracted into the lower diagonal block, and replaced with zeros in the upper, positive block (blue).}
    \label{fig:block_diagonal_adjacency_matrix}
\end{figure}

\section{Models for graph classification}

Having described the preprocessing of the brain graphs, we now turn to the models used to perform graph prediction for individual subjects. Two models will be presented, a baseline regression model and a \acrshort{gcn} model, for the tasks of predicting subject age and sex.

\subsection{Baseline}
To be able to validate how well the different \acrshort{gcn}-models perform, a baseline model must be introduced for comparison. In this study, the baseline model consisted of a regression model where the connections between all of the nodes were regarded as separate input features. The adjacency matrix for an individual subject could then be viewed as a high dimensional data point and the model thus aimed to fit a hyper plane to either separate the classes or predict a continuous variable. For a schematic view of the model see figure \ref{fig:Graph_class_baseline}. Note that the output layer has either two softmax-activated neurons in the case of sex classification or one neuron without activation in the case of age regression. 

From the definition of a graph in \ref{sec:general_graph_theory}, it follows that graphs are completely node order invariant, and several different ways of listing the connections results in the same graph. This might generally impose a problem since the regression model depend on its input being ordered. A regression model is however possible as a baseline model, since the brain graphs from the UK Biobank follow a consistent node ordering, and each input feature of the model is thus always the same connection. Furthermore, the regression model is also possible due to the relatively small size of the brain graphs (21 nodes) compared to many other graphs (such as citation networks consisting of thousands of nodes). For larger graphs, models with one learnable parameter per connection would result in huge models. In that case, models that do not scale with the number of connections must be used, such as \acrshort{gcn}-based models. 

% \begin{center}
%     \resizebox {0.9\linewidth} {!} {
%         \input{chapters/images_methods/baseline_ffnn.tikz}
%     }
% \end{center}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{chapters/images_methods/ffnn_v2.png}
    \caption{The baseline regression model. The adjacency matrix for a brain graph for a single subject forms the input, with a continuous value or class prediction as output in the case of sex or age prediction.}
    \label{fig:Graph_class_baseline}
\end{figure}

\subsection{GCN}

The motivation behind using \acrshort{gcn}-based models is that they hopefully will be able to extract information from the topological structure of the graph, rather than simply studying the individual connections as, for instance, in the case of the baseline model. To explore this possibility, a \acrshort{gcn}-based model, referred to simply as GCN, was developed. An illustration of the GCN model can be seen in \cref{fig:gcn_base}. GCN consisted of three consecutive graph convolutional layers, with propagation rule as defined in equation \eqref{eq:propagation_rule}, followed by a fully connected output layer. The graph convolutional layers had a \acrfull{relu} activation function and ten output features. This will be the case for all graph convolutional layers mentioned in the remainder of the thesis, unless otherwise specified. The input to the fully connected output layer consisted of the activations for all three graph convolutional layers, i.e. the activation for all feature maps in all layers, concatenated together. As described in \cref{sec:gcn} the activation of layer $i$ contains information about the $i$:th-order neighbourhood of each node. The inclusion of the activations after each layer in the classifier thus aims to utilize information of how each node is embedded in a successively larger neighbourhood, which could be beneficial for the prediction.

The inputs to a graph convolutional model generally consists of an adjacency matrix $A$ and a node feature matrix $X$. However, the brain graphs in this thesis does not contain any information beyond connections that can be associated with specific nodes. GCN thus uses a \textit{featureless} approach, in which the feature matrix was taken to be the identity matrix, $X=I$.

% \begin{center}
%     \resizebox {0.9\linewidth} {!} {
%         \input{chapters/images_methods/gcn.tikz}
%     }
% \end{center}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{chapters/images_methods/base_v2.png}
    \caption{The GCN model, which takes a single brain graph as input and outputs a predicted age or sex. The input graph is processed through three graph convolutional layers with ReLU activation, afterwhich the activations after each layer is concatenated together and fed into a fully connected layer.}
    \label{fig:gcn_base}
\end{figure}


\section{Models for node classification}

With the models performing graph predictions introduced, two models used to perform node prediction on a population graph will now be presented. First, however, the procedure of forming population graphs will be described, followed by a batching method that enables models to be trained.

\subsection{Forming population graphs}

As discussed in section \ref{sec:similarity_measure}, the design of the similarity measure is an important decision and should be done with the given application in mind. For our specific application, it's desirable that subjects that have similar \acrshort{fmri} data are connected with edges that have large weights, with the idea that the models will be able to draw upon this information of similarity to yield a better inference. A general construction that fulfils this requirement is the use of a distance metric inverted by a kernel as described in Equation \eqref{eq:similarity_measure}. Following this approach, and given two subjects and their adjacency matrices $A_1$ and $A_2$, we define the similarity measure $\sigma\left(A_1, A_2, l\right)$ as
\begin{equation}
    \sigma\left(A_1, A_2, l\right) = \exp{\left(- \frac{||A_1 - A_2||_F^2}{l||A_1||_F ||A_2||_F} \right)}\biggr\rvert_{l=0.5},
    \label{eq:modified_similarity_measure}
\end{equation}
where $||A_1 - A_2 ||_F$ is the matrix Frobenius norm of the difference between $A_1$ and $A_2$. The Frobenius norm is defined as $||A||_F = \left( \sum_i \sum_j |A_{ij}|^2 \right)^{1/2}$. The norm of the difference was weighted with a hyperparameter $l=0.5$ and the norms of $A_1$ and $A_2$, and then fed into a Gaussian kernel. The Gaussian kernel ensures that larger differences between $A_1$ and $A_2$ yields smaller similarity scores $\sigma\left(A_1, A_2, l\right)$, and also that  $\sigma\left(A_1, A_2, l\right) \in \left[0, 1\right]$. As desired, subjects that have similar fMRI data, and thus a smaller difference between their adjacency matrices, will with Equation \eqref{eq:modified_similarity_measure} obtain a larger similarity score and vice versa. 

Note that the similarity measure in equation \eqref{eq:modified_similarity_measure} only utilises the \acrshort{fmri} data for each subject. One could imagine a similarity measure that uses other types of data that is related to the tasks of predicting age/sex, such as eventual brain-health related diagnosis \cite{stankeviciute}. The choice of only using \acrshort{fmri} data was motivated by two reasons; partly because using other data sources requires extensive domain knowledge, and partly because we are specifically interested in the predictive power of \acrshort{fmri} data, without introducing other confounding variables.

\subsection{Poptoy model}
As a first model for predictions on a population graph the Poptoy model is introduced. Poptoy takes in a population graph and propagates it through five graph convolutional layers, similar to GCN, but the output layer differs as Poptoy performs node prediction instead of graph prediction. The output layer of Poptoy is still a fully connected layer with one or two softmax output neurons, but the input to the fully connected layer consists of the activations of all layers and all features for a specific node. This is in contrast to GCN's output layer which takes the activations for all layers and all features for all nodes as input. Then, using the same fully connected output layer for each node repeatedly (same weights), a prediction for each node, and thus each subject, can be made and the number of weights is kept low as the number of nodes grows. An illustration of the Poptoy model can be seen in figure \ref{fig:poptoy}.

The reason the Poptoy model consisted of five graph convolutional layers instead of three, which GCN had, was that the population graphs became much larger then the individual brain graphs. The individual brain graphs consisted of 21 nodes which where all more or less connected and since each graph convolutional layer takes into account one higher order of neighbours, the need for more graph convolutional layers quickly diminishes, as all nodes to some extent are included in the first order neighbourhood. For population graphs the need for considering neighbours further away in the graph might thus be much larger and hence Poptoy utilizes more graph convolutional layers than GCN. Five layers were specifically chosen since adding more layers did not increase performance.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{chapters/images_methods/poptoy_v2.png}
    \caption{The PopToy model. The input consists of a population graph, which is passed through five graph convolutional layers. The five activations are concatenated together and fed into a fully connected output layer, which outputs a predicted age or sex on a subject-level.}
    \label{fig:poptoy}
\end{figure}

 The split into validation and training set must be handled, since all subjects in the data set are incorporated in the population graph. The solution is given by defining a set of subjects that will be the training set and another that will be the validation set. The population graph thus has one set of nodes considered to be training nodes and one set of validation nodes. The model was then constructed so it could either do predictions for only the nodes in the training set or validation set. Then, the training could be performed while only doing predictions on the training set and vice versa when evaluating.

%\subsection{POPToy w. dummy}
%\subsection{POPToy w. features (fmri)}
\subsection{POPEncoder}
As a means to incorporate more information about each subject in the population graph, a model referred to as POPEncoder was designed. The Popencoder model is identical to Poptoy in the sense that it propagates the population graph through five graph convolutional layers and then makes a prediction for each node with a fully connected layer. The difference is that in Popencoder, features for each node are introduced. The features are based on the adjacency matrices of each individual subject, but to compress the dimensionallity of the feature space these matrices were encoded into a lower dimensional space. The encoder consisted of two graph convolutional layers followed by a fully connected layer with two output neurons and a linear activation function. For an illustrations see figure \ref{fig:popencoder}.

A heuristic explanation of why the features introduced to the population graph would help, is that the encoder can make an initial prediction of the age or sex of each individual subject. Then, by propagating this information through the population graph, the information of similarities to other subjects in the data set possibly can improve on the predictions. In that case, one could argue that the initial prediction might be done as a preprocessing step, for example via a prediction with another model. The encoding is, however, viewed as a trainable part of the model and has two output features to allow for more abstract and advantageous embeddings to be learnt during model training. 



\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{chapters/images_methods/popencoder_v2.png}
    \caption{The PopEncoder model, which takes two inputs: a population graph $A$, and the brain graphs for all subjects in the population graph, $X$. The brain graphs are passed through an Encoder, consisting of two graph convolutional layers followed by a fully connected layer. The encoded brain graphs and the population graph $A$ are then fed into the Classifier, which consists of five graph convolutional layers followed by a concatenation and fully connected layer. The output is a predicted age or sex on a subject-level.}
    \label{fig:popencoder}
\end{figure}

\section{Batches of population graphs}
The adjacency matrix for the population graph quickly becomes very large, since the number of edges in the graph grows with the number of subjects squared. For example, a population graph with 30 000 subjects requires approximately 7 GB of memory to store. This is a problem since doing prediction and training with such a large matrix is time consuming and the memory consumption can also be problematic. To resolve this problem, and thus in a feasible way be able to train models on population graphs including all the data, a batching method was developed. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{chapters/images_methods/batching.png}
    \caption{An example of the batching approach for splitting a large population graph $A_{population}$ into two smaller population graphs, each corresponding to a batch $A_{batch}$. In Epoch 1, only the connections between the first three (blue) and last three (orange) subjects are included in the batched population graphs. By permuting which subjects are included in each of the smaller population graphs for Epoch 2 and onwards, all connections not included in the first epoch (gray) will eventually be sampled.}
    \label{fig:batches}
\end{figure}

The batching method was based on dividing the population graph into several smaller population graphs. This was done by splitting the data set into several smaller data sets of 100 subjects each and constructing one population graph for each smaller data set. Practically this was done by extracting all connections between the 100 subjects from the larger population graph. For an illustration on how this was done see \cref{fig:batches}. With this batching approach, dividing a population graph of 30 000 subjects into for instance 300 smaller graphs with 100 nodes each would require 24 MB of memory, as compared to 7 GB before batching. This is a difference of roughly two orders of magnitude.

As seen in \cref{fig:batches} many of the connections between subjects are not utilized if the population graph is divided into several smaller graphs. To solve this, the way the subjects are divided into smaller graphs is changed between epochs as seen in \cref{fig:batches}. By always changing which people are combined in the graphs, all connections will eventually be used after enough epochs. The advantage of always changing the subjects that are included in the graphs is that the model can't overfit against a specific graph structure. The models are thus forced to learn very general patterns to make predictions for all subjects in the graphs. However, this generalization might also become a disadvantage since overfitting on the graph structure may yield higher validation performance as long as the graph structure remains fixed. 


\section{Model explainability through saliency mapping}
The goal of the thesis is not only to construct models to achieve good performance, but also to determine what functional networks in the brain are related to sex and age. This means that model analysis, in the form of analysing what the models have learned, is an essential part of the work. Specifically, analysing which nodes in the brain graph are important for making predictions will by extension give information on what functional networks are important, due to each node representing a functional network. 

Two methods for model analysis will be used: a naive approach based on node removal, and a more sophisticated method for node masking based on the Zorro algorithm described in \cref{sec:zorro}. Note that both these methods are not methods that open up the black-box of neural networks per say (by e.g. parameter analysis), that can be used for the "explainable AI" concept.  These methods analyse/explain the networks from an outer perspective, which is beneficial as it poses no assumption on the model architecture, only in- and output data. Thus, the same analysis flow may be performed for different models, making it also beneficial for validation and result comparison. 

\subsection{Naive node removal}
A simple method to investigate which nodes that are important for predictions is a node removal method. This particular method is self-composed, and based on simply removing a specific node for every subject in the data set and then retraining a new model. By comparing the predictive performance of the retrained model with a reference model trained on data with no nodes removed, an indication of the importance of that node may be obtained. This is the case since a large loss in performance means vital information for the prediction was removed, which is interpreted to be indicative of the importance of that node. The method may then be repeated for all nodes in order to obtain a measure of how important each node is.

\subsection{Zorro}

The Zorro algorithm, described in \cref{sec:zorro}, was developed for analysis of semi-supervised learning models on a node classification task. It thus needed to be modified to be applied to the models presented in this thesis.

The need for modification arose since our Baseline and GCN models are completely featureless approaches that only take an adjacency matrix $A$ as input. Therefore, it is not possible to introduce noise to the feature matrix, as done in the original method, and it must be done on $A$. The reintroduction of nodes in $A$, in the following referred to as unmasking, can be done in two ways; either on a connection level where entries in $A$ are unmasked, or on a nodal level were whole rows and columns in $A$ are unmasked. We were primarily interested in which nodes are important, so the latter was used. Unmasking whole nodes instead of connections also gives computational benefits, since the number of nodes is less then the number of connections. As whole nodes are unmasked, the explanation in the modified Zorro method only contains a set of nodes, $\mathcal{S} = \{V\}$. The masked adjacency matrix is now given by 
\begin{equation*}
    B_S = A \odot S + Z \odot (1- S), \quad Z \sim \mathcal{N},
\end{equation*}
where $S$ is the masking matrix for the explanation $\mathcal{S}$, and the model predictions are $\Phi(A)$ and $\Phi(B_S)$. For age prediction, $\Phi(B_S)$ was deemed correct if $\left|\Phi(A) - \Phi(B_S)\right| < t$ for some tolerance $t$, since age is a continuous variable. The noise is drawn from a Gaussian distribution with mean and standard deviation given by the entries of $A$ over the data set. The fidelity was calculated in the same way as in the original algorithm, and an explanation $\mathcal{S}$ for an individual subject was still accepted if the fidelity of $\mathcal{S}$ was higher then $\tau$. 

Lastly, since the algorithm yields which nodes are important for the prediction of an individual subject, the procedure was repeated for several subjects to get a sense of which nodes are generally important. To evaluate the importance of each node an importance score $\mathcal{I}$ was introduced as the number of explanations $\mathcal{S}$ a node was included in, divided by the total number of subjects. An importance score of $\mathcal{I}=1$ indicates that the node was considered important for the prediction of all subjects, and a score of $\mathcal{I}=0$ for none.  