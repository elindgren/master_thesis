\chapter{Method}
\label{chap:method}

In this chapter, we present the methodology for how accurate models for predicting age and sex were developed, and how they were analysed using saliency mapping techniques in order to gain insight into which functional brain networks in the data are related to age and sex. The chapter can be summarised as follows: firstly, we present how the \acrshort{fmri} data was preprocessed into graphs suitable for graph neural networks. Secondly, the models developed for graph and node prediction are presented, followed by the methods used for analysing and creating saliency maps from the models. See \cref{app:model_training} for more details on the exact model architectures used. 

\section{From RS-fMRI scans to graphs}\label{sec:fmri_to_graphs}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{chapters/images_methods/fmri_network_ukbiobank.png}
    \caption{A schematic visualisation of a network of functional brain networks. Blue and orange edges represent positive and negative correlation between brain networks, respectively.  The individual functional brain networks are represented as human brains with \acrshort{fmri} activations (orange), along the circumference of the circle. See \cref{tab:Networks} for the names of the functional brain networks. Image adapted from example at UK Biobank Brain Imaging Online Resources \cite{ukbiobank_brain_imaging}.}
    \label{fig:fmri_network}
\end{figure}

The graphs of functional brain networks used in this thesis are derived from \acrshort{fmri}. In short, functional \acrshort{mri} (\acrshort{fmri}) is a technique for measuring the neuronal activity in the human brain \cite{sporns}. In a measurement, the activations of various parts of the subject's brain are measured as time series. The correlations between these time series can then be calculated and formed into a network. Different sub-networks in the obtained \acrshort{fmri} brain network can often be associated with different functionalities of the brain. One example of such a functional brain network is the \acrfull{dmn}, which handles memory processing and mind wandering  \cite{alves_dmn}. The regions that make up the sub-networks do not necessarily have to be physically close together nor directly anatomically connected.


\begin{table}[!htbp]
    \centering
    \caption{A list of which node corresponds to which functional brain network in the data from the UK Biobank. See \cref{fig:fmri_network} for a representation of each functional brain network. Each node will in the remainder of this thesis be referred to by the abbreviation for its functional brain network.}
    \begin{tabular}{||c|c|c||}
        \hline
        Number & Network & Abbreviation  \\ \hline\hline
        1 & \acrlong{cb1} & \acrshort{cb1} \\ \hline
        2 & \acrlong{vl} & \acrshort{vl}  \\ \hline
        3 & \acrlong{ssn} & \acrshort{ssn} \\ \hline
        4 & \acrlong{vm} &  \acrshort{vm}\\ \hline
        5 & \acrlong{dar} & \acrshort{dar} \\ \hline
        6 & \acrlong{dal} &  \acrshort{dal} \\ \hline
        7 & \acrlong{fp} &  \acrshort{fp}\\ \hline
        8 & \acrlong{vv1m} & \acrshort{vv1m} \\ \hline
        9 & \acrlong{dmn} &  \acrshort{dmn} \\ \hline
        10 & \acrlong{pss} & \acrshort{pss} \\ \hline
        11 & \acrlong{pmn} & \acrshort{pmn} \\ \hline
        12 & \acrlong{smm} & \acrshort{smm}  \\ \hline
        13 & \acrlong{va} &  \acrshort{va}\\ \hline
        14 & \acrlong{sn} &  \acrshort{sn}\\ \hline
        15 & \acrlong{cb2} & \acrshort{cb2} \\ \hline
        16 & \acrlong{pl} & \acrshort{pl} \\ \hline
        17 & \acrlong{ts} & \acrshort{ts} \\ \hline
        18 & \acrlong{bg} &  \acrshort{bg} \\ \hline
        19 & \acrlong{vv1l} & \acrshort{vv1l} \\ \hline
        20 & \acrlong{pmc} & \acrshort{pmc} \\ \hline
        21 & \acrlong{tm} & \acrshort{tm} \\ \hline
    \end{tabular}
    \label{tab:Networks}
\end{table}


This thesis utilises \acrshort{fmri} data obtained form the UK Biobank for roughly 35000 subjects, of both sexes and varying between 45 and 80 years of age \cite{ukbiobank}. The \acrshort{fmri} data had been preprocessed and consisted of correlations between 21 different functional brain networks. The correlation between these 21 networks thus constituted a network of networks, and is interpreted as a graph where each node corresponds to one functional network and each edge the correlation between networks. A visualisation of such a brain graph is given in \cref{fig:fmri_network}, in which the various nodes and the functional brain networks they represent form the outer circle, with the coloured lines connecting the nodes being the edges in the graph. The lines are coloured according to the edge weights, with blue and orange lines having positive and negative weights respectively. For a list of what functional brain network each node corresponds to, and its abbreviation, see \cref{tab:Networks}.

\subsection{Treating negative edge weights}
A brain graph with both positive and negative weights may be problematic. Specifically, when normalising the adjacency matrices according to Equation \eqref{eq:renormalization_trick}, the risk of dividing by zero becomes imminent. There are several ways to handle this problem. One alternative is to only study all positive or all negative connections, and another might be to take the absolute value of all connections. We, however, decided to split the negative and positive connections into two separate graphs and thus form a multiplex graph. From a practical point of view this was implemented by creating a block diagonal adjacency matrix with all positive connections in the upper block and all negative connections in the lower block, see \cref{fig:block_diagonal_adjacency_matrix}. The connections in the positive block that were negative in the original adjacency matrix $A$ were replaced with zeros, and vice versa in the negative block.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{chapters/images_methods/split.png}
    \caption{An example of how negative values in an adjacency matrix $A$ were handled. The negative values (orange) were extracted into the lower diagonal block of $A_{split}$, and replaced with zeros in the upper, positive block (blue).}
    \label{fig:block_diagonal_adjacency_matrix}
\end{figure}

\section{Models for graph prediction}

Having described the preprocessing of the brain graphs, we now turn to the models used to perform graph prediction for individual subjects. Two models will be presented, a baseline regression model and a \acrshort{gcn} model, for the tasks of predicting subject age and sex.

\subsection{Baseline}
To validate how well the different \acrshort{gcn}-models performed, a baseline model had to be introduced for comparison. In this study, the baseline model consisted of a regression model where the connections between all of the nodes were regarded as separate input features. The adjacency matrix for an individual subject could then be viewed as a high dimensional data point, and the model thus aimed to fit a hyper plane to either separate the classes or predict a continuous variable. For a schematic view of the model see \cref{fig:Graph_class_baseline}. Note that the output layer has either two softmax-activated neurons in the case of sex classification, or one neuron without activation in the case of age regression. 


\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{chapters/images_methods/ffnn_v2.png}
    \caption{The baseline regression model. The adjacency matrix for a brain graph for a single subject forms the input, with a continuous value or class prediction as output in the case of age or sex prediction.}
    \label{fig:Graph_class_baseline}
\end{figure}


From the definition of a graph in \cref{sec:general_graph_theory}, it follows that graphs are completely node order invariant, and several different ways of listing the connections results in the same graph. This might generally impose a problem since a regression model depends on its input being ordered. In this thesis, a regression model was, however, possible as a baseline model, since the brain graphs from the UK Biobank followed a consistent node ordering, and each input feature of the model was thus always the same connection. Furthermore, the regression model was feasible due to the relatively small size of the brain graphs (21 nodes) compared to many other graphs (such as citation networks consisting of thousands of nodes). For larger graphs, models with one learnable parameter per connection would result in huge models. In that case, models that do not scale with the number of connections must be used, such as \acrshort{gcn}-based models. 

% \begin{center}
%     \resizebox {0.9\linewidth} {!} {
%         \input{chapters/images_methods/baseline_ffnn.tikz}
%     }
% \end{center}

\subsection{GCN}

The motivation behind using \acrshort{gcn}-based models was that they hopefully would be able to extract information from the topological structure of the graphs, rather than simply studying the individual connections as, for instance, Baseline. A \acrshort{gcn}-based model, referred to simply as GCN, was thus developed. An illustration of the GCN model can be seen in \cref{fig:gcn_base}. GCN consisted of three consecutive graph convolutional layers, with propagation rule as defined in equation \eqref{eq:propagation_rule}, followed by a fully connected output layer. The graph convolutional layers had a \acrfull{relu} activation function and ten output features. This will be the case for all graph convolutional layers mentioned in the remainder of this thesis, unless otherwise specified. The input to the fully connected output layer consisted of the activations for all three graph convolutional layers, i.e. the activation for all feature maps in all layers, concatenated together. As described in \cref{sec:gcn} the activation of layer $i$ contains information about the $i$:th-order neighbourhood of each node. The inclusion of the activations after each layer in the classifier thus aimed to utilise information of how each node was embedded in a successively larger neighbourhood, which could be beneficial for the predictions.

The inputs to a graph convolutional model generally consists of an adjacency matrix $A$ and a node feature matrix $X$. However, the brain graphs in this thesis did not contain any information beyond connections that could be associated with specific nodes. GCN thus used a \textit{featureless} approach, in which the feature matrix was taken to be the identity matrix, $X=I$ \cite{kipf_vae}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{chapters/images_methods/base_v2.png}
    \caption{The GCN model, which takes a single brain graph as input and outputs a predicted age or sex. The input graph is processed through three graph convolutional layers with ReLU activation, after which the activations for each layer are concatenated together and fed into a fully connected layer.}
    \label{fig:gcn_base}
\end{figure}


% \begin{center}
%     \resizebox {0.9\linewidth} {!} {
%         \input{chapters/images_methods/gcn.tikz}
%     }
% \end{center}



\section{Models for node prediction}

With the models performing graph predictions introduced, two models used to perform node prediction on a population graph will now be presented. First, however, the procedure of forming population graphs will be described, followed by a batching method that enables models to be trained.

\subsection{Forming population graphs}

As discussed in \cref{sec:similarity_measure}, the design of the similarity measure is an important decision and should be done with the given application in mind. For our specific application, it's desirable that subjects that have similar \acrshort{fmri} data are connected with edges that have large weights, with the idea that the models will be able to draw upon this information of similarity to yield a better inference. A general construction that fulfils this requirement is the use of a distance metric inverted by a kernel, as described in Equation \eqref{eq:similarity_measure}. With this construction, and given two subjects and their adjacency matrices $A_1$ and $A_2$, we defined the similarity measure $\sigma\left(A_1, A_2, l\right)$ as
\begin{equation}
    \sigma\left(A_1, A_2, l\right) = \exp{\left(- \frac{||A_1 - A_2||_F^2}{l||A_1||_F ||A_2||_F} \right)}\biggr\rvert_{l=0.5},
    \label{eq:modified_similarity_measure}
\end{equation}
where $||A_1 - A_2 ||_F$ is the matrix Frobenius norm of the difference between $A_1$ and $A_2$. The Frobenius norm is defined as $||A||_F = \left( \sum_i \sum_j |A_{ij}|^2 \right)^{1/2}$. The norm of the difference was weighted with a hyperparameter $l=0.5$ and the norms of $A_1$ and $A_2$, and then fed into a Gaussian kernel. The Gaussian kernel ensured that larger differences between $A_1$ and $A_2$ yielded smaller similarity scores $\sigma\left(A_1, A_2, l\right)$, and also that  $\sigma\left(A_1, A_2, l\right) \in \left[0, 1\right]$. As desired, subjects that had similar fMRI data, and thus a smaller difference between their adjacency matrices, obtained a larger similarity score and vice versa. 

% Note that the similarity measure in equation \eqref{eq:modified_similarity_measure} only utilises the \acrshort{fmri} data for each subject. One could imagine a similarity measure that uses other types of data that is related to the tasks of predicting age/sex, such as eventual brain-health related diagnosis \cite{stankeviciute}. The choice of only using \acrshort{fmri} data was motivated by two reasons; partly because using other data sources requires extensive domain knowledge, and partly because we are specifically interested in the predictive power of \acrshort{fmri} data, without introducing other confounding variables.

\subsection{Batches of population graphs}
The adjacency matrix for a population graph quickly becomes very large, since the number of edges in the graph grows with the number of subjects squared. For example, a population graph with 30 000 subjects requires approximately 7 GB of memory to store. This is a problem, since predicting and training using such a large matrix is time consuming, and the memory consumption can also be problematic. To resolve this problem, and thus in a feasible way be able to train models on population graphs including all the available data, a batching method was developed. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{chapters/images_methods/batching.png}
    \caption{An example of the batching approach for splitting a large population graph $A_{population}$ into two smaller population graphs, each corresponding to a batch $A_{batch}$. In the first epoch, only the connections between the first three (blue) and last three (orange) subjects are included in the batched population graphs. By permuting which subjects are included in each of the smaller population graphs for the second epoch and onwards, all connections not included in the first epoch (gray) will eventually be sampled.}
    \label{fig:batches}
\end{figure}

The batching method was based on dividing the population graph into several smaller population graphs. This was done by splitting the data set into several smaller data sets of 100 subjects each and constructing one population graph for each smaller data set. In practice, this was done by extracting all connections between the 100 subjects from the larger population graph. For an illustration on how this was done see \cref{fig:batches}. With this batching approach, dividing a population graph of 30000 subjects into, for instance, 300 smaller graphs with 100 nodes each would require 24 MB of memory, as compared to 7 GB before batching. This is a difference of roughly two orders of magnitude.

As seen in \cref{fig:batches}, many of the connections between subjects are not utilised if the population graph is divided into several smaller graphs. To solve this, the way the subjects were divided into smaller graphs was changed between epochs, as seen in \cref{fig:batches}. By always changing which people were combined in the graphs, all connections were eventually used after enough epochs. The advantage of always changing the subjects that were included in the graphs was that the models could not overfit against a specific graph structure. The models were thus forced to learn very general patterns to make predictions for all subjects in the graphs. However, this generalisation might also be a disadvantage, since overfitting on the graph structure may yield higher validation performance as long as the graph structure remains fixed. 

\subsection{Poptoy}

As a first model for predictions on a population graph, the Poptoy model was introduced. The input to Poptoy consisted of a population graph, which was then propagated through five graph convolutional layers. The architecture of Poptoy was similar to that of GCN, but the output layers differed. The output layer of Poptoy was a fully connected layer, and took the activations of all layers and all features for a specific node as input. GCN also had a fully connected output layer, but it took the activations for all layers and all features for all nodes as input. Since the fully connected layer for Poptoy outputs a prediction for a single node, it could be reused for all nodes and thus the number of weights was kept low as the number of nodes in the population graph grew. An illustration of the Poptoy model can be seen in \cref{fig:poptoy}.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\textwidth]{chapters/images_methods/poptoy_v2.png}
    \caption{The Poptoy model. The input consists of a population graph, which is passed through five graph convolutional layers. The five activations are concatenated together and fed into a fully connected output layer, which outputs a predicted age or sex on a subject-level.}
    \label{fig:poptoy}
\end{figure}

The reason the Poptoy model consisted of five graph convolutional layers instead of three, which GCN had, was that the population graphs became much larger than the individual brain graphs. The individual brain graphs consisted of 21 nodes, which were all more or less connected, and since each graph convolutional layer takes into account one higher order of neighbours, all nodes were to some extent included in the first order neighbourhood. Thus, the need for more graph convolutional layers quickly diminished. For population graphs, the need for considering neighbours farther away in the graph might be much larger, and hence Poptoy utilised more graph convolutional layers than GCN. Five layers were specifically chosen since adding more layers did not increase performance.

Furthermore, since all subjects in the data set were incorporated in the population graph, the split into validation and training sets had to be handled. To solve this, a set of subjects in the population graph were defined to be the training set, and the rest to be the validation set. The model was then constructed to either do predictions for only the nodes in the training set or validation set. Then, the training could be performed while only doing predictions on the set and vice versa when evaluating. 




\subsection{Popencoder}
As a means to incorporate more information about each subject in the population graph, a model referred to as Popencoder was designed. Popencoder is identical to Poptoy in the sense that the population graph was propagated through five graph convolutional layers, after which a prediction for each node was made using a fully connected layer. The difference is that in Popencoder, features for each node were introduced. The features were based on the adjacency matrices of each individual subject, but to compress the dimensionallity of the feature space these matrices were encoded into a lower dimensional space. The encoder consisted of two graph convolutional layers followed by a fully connected layer with either two softmax activated neurons or one linearly activated neuron, in the case of sex or age prediction respectively. For an illustration of Popencoder, see \cref{fig:popencoder}.

A heuristic explanation of why the features introduced to the population graph would help, is that the encoder can make an initial prediction of the age or sex for each individual subject. Then, by propagating this information through the population graph, the information of similarities to other subjects in the data set possibly can improve on the predictions. In that case, one could argue that the initial prediction might be done as a preprocessing step, for example via a prediction with another model. The encoding is, however, viewed as a trainable part of the model to allow for more abstract and advantageous embeddings to be learnt during model training. 



\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{chapters/images_methods/popencoder_v2.png}
    \caption{The Popencoder model, which takes two inputs: a population graph $A$, and the brain graphs for all subjects in the population graph, $X$. The brain graphs are passed through an encoder, consisting of two graph convolutional layers followed by a fully connected layer. The encoded brain graphs and the population graph $A$ are then fed into a classifier, which consists of five graph convolutional layers followed by a concatenation and fully connected layer. The output is a predicted age or sex on a subject-level.}
    \label{fig:popencoder}
\end{figure}

\section{Model explainability through saliency mapping}
To determine what functional networks in the brain are related to sex and age, model analysis, in the form of analysing what the models have learned, is an essential part of the work. Specifically, using saliency mapping techniques to analyse which nodes in the brain graph are important for making predictions will by extension give information on what functional networks are important, due to each node representing a functional network. 

Two methods for model analysis was used: a naive approach based on node removal, and a more sophisticated method for node masking based on the Zorro algorithm described in \cref{sec:zorro}. Note that these are not methods that open up the black-box of neural networks per say (by e.g. parameter analysis). These methods analyse/explain the networks from an outer perspective, which is beneficial as it poses no assumption on the model architecture, only in- and output data. Thus, the same analysis may be performed for different models, making it beneficial for validation and result comparison. 

\subsection{Naive node removal}
A simple method to obtain a saliency map of which nodes are important for predictions is a node removal method. This particular method is self-composed, and consisted of simply removing a specific node for every subject in the data set, after which a new model was retrained. By comparing the predictive performance of the retrained model with a reference model trained on data with no nodes removed, an indication of the importance of that node could be obtained. This is based on the assumption that a large loss in performance means vital information for the prediction had been removed, which was interpreted to be indicative of the importance of that node. The method was then repeated for all nodes in order to obtain a measure of importance for each node.


\subsection{Zorro}

The Zorro algorithm, described in \cref{sec:zorro}, was developed for saliency mapping of models with graph features as input. It thus needed to be modified to be applied to the models presented in this thesis. The need for modification arose since our Baseline and GCN models are completely featureless approaches that only takes an adjacency matrix $A$ as input. Therefore, it was not possible to introduce noise to the feature matrix, as done in the original method. Instead, it had to be done on $A$. The reintroduction of nodes in $A$, in the following referred to as unmasking, could have been done in two ways; either on a connection level where entries in $A$ are unmasked, or on a nodal level where whole rows and columns in $A$ are unmasked. We were primarily interested in which nodes were important, so the latter was used. Unmasking whole nodes instead of connections also yielded computational benefits, since the number of nodes was less than the number of connections. As whole nodes were unmasked, the explanation in the modified Zorro method only contained a set of nodes, $\mathcal{S} = \{V\}$. The masked adjacency matrix was then given by 
\begin{equation}
    B_S = A \odot S + Z \odot (1- S), \quad Z \sim \mathcal{N},
\end{equation}
where $S$ is the masking matrix for the explanation $\mathcal{S}$, and the model predictions are $\Phi(A)$ and $\Phi(B_S)$. For age prediction, $\Phi(B_S)$ was deemed correct if 
\begin{equation}
    \left|\Phi(A) - \Phi(B_S)\right| < t
\end{equation}
for some tolerance $t$, since age is a continuous variable. The noise $Z$ was drawn from a Gaussian distribution with mean and standard deviation given by the entries of $A$ over all subjects in the data set. The fidelity was calculated in the same way as in the original algorithm, and an explanation $\mathcal{S}$ for an individual subject was still accepted if the fidelity of $\mathcal{S}$ was higher than $\tau$. 

Lastly, since the algorithm yielded which nodes were important for the prediction of an individual subject, the procedure was repeated for several subjects to get a sense of which nodes were generally important. To evaluate the importance of each node, an importance score $\mathcal{I}$ was introduced as the number of explanations $\mathcal{S}$ a node was included in, divided by the total number of subjects. An importance score of $\mathcal{I}=1$ indicates that the node is considered important for the prediction of all subjects, and a score of $\mathcal{I}=0$ for none.  