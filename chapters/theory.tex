\chapter{Theory}

\section{General graph theory}\label{sec:general_graph_theory}

A graph $\mathcal{G} = \{V, E \} $ is defined by the set of its nodes $V$ and edges $E$. The edges forms the connections between nodes, and for a pair of nodes $(i,j)$ the corresponding edge may be denoted $e_{ij}$. We may define the neighbours to a certain node $v \in V$ as $N(v)$, which thus are the nodes $u \in V$ that are connected to $v$, $e_{vu} \neq 0$. Each node $v$ may also be associated with \textit{node features}, $x_v$. $x_v$ could be a vector, and can be seen as the properties of a node.

A graph is considered to be \textit{undirected} if the edges are undirected, $e_{ij} \equiv e_{ji}$, and \textit{directed} if the edges are directed, $e_{ij} \not\equiv e_{ji}$. Furthermore, a graph can either be \textit{binary} or \textit{weighted}. In a binary graph, edges are either present or not, whilst in a weighted graph each edge $e_{ij}$ is associated with a weight $w_{ij}$. \cite{source} Weights in a weighted graph may be interpreted differently depending on the application. They can be seen as the connection strength between devices in a cellular network, the flow of goods in a logistics setting, a distance metric etc.

The edges in a graph may be succinctly summarised in an \textit{adjacency matrix} $A$. $A$ is defined as

\begin{equation}
    A_{ij} = \begin{cases} \mbox{1,} & \mbox{if } e_{ij} \in E \\ \mbox{0,} & \mbox{otherwise} \end{cases}
    \label{eq:adjacencydefinition}
\end{equation}
In the case of a weighted graph, $A_{ij} = w_{ij}$. From definition \eqref{eq:adjacencydefinition} it follows that undirected graphs have a symmetric adjacency matrix, $A_{ij} = A_{ji}$. In this work we mainly focus on the edges rather than the nodes of the brain networks, and thus adjacency matrices will be our primary way of representing graphs.

Another kind of graph which will be utilized in the thesis is multiplex graphs. A multiplex graph is a graph with a fixed set of nodes but with several sets of edges. One example of a multiplex graph is a graph with cities as nodes and different means of transportation as edges. Edges representing highways and railways makes up two different graphs but they might be combined into a multiplex graph since they have the same nodes.

% \subsection{Graph classification}
% \subsection{Node classification}
% \subsection{regression}



\section{Graph Convolutional Neural Networks}
\label{sec:gcn}

Graph Convolutional Neural Networks (GCNs) are a class of neural networks that generalize the notion of convolutions from grid data to graph structured data \cite{wu_review}. Regular convolutional operations typically operate on structured grids of data, for instance the pixels in an image, where each grid point is only connected to its adjacent neighbours. This may not be the case in graph structured data, where any pair of nodes can be connected. This difference can be visualised as in figure \ref{fig:grid_graph_data}. 

\begin{figure}
    \centering
        \begin{subfigure}{.5\textwidth}
            \centering
            \includegraphics[width=.9\linewidth]{chapters/images_theory/placeholder.jpg}
            \caption{Grid data}
            \label{fig:grid_data}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
            \begin{center}
                \resizebox {0.9\linewidth} {!} {
                    \input{chapters/images_theory/graph.tikz}
                }
            \end{center}
            \caption{Graph data}
            \label{fig:graph_dad}
        \end{subfigure}
    \caption{Comparison between grid structured and graph structured data }
    \label{fig:grid_graph_data}
\end{figure}

We begin this section by first discussing the concept of convolutions in the graph domain, and how they can implemented in a computationally efficient manner. We then present a layer-wise propagation rule for a graph convolutional layer based on this implementation of convolutions. Finally, we discuss graph convolutional layers from the viewpoint of a heuristic interpretation known as \textit{message passing}.

\subsection{Convolutions in the graph domain}

Consider an undirected graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ with $N$ nodes and adjacency matrix $A$. The degree matrix $D$ to $\mathcal{G}$ is defined as 
\begin{equation}
    D_{ij} = \begin{cases} \sum_j A_{ij}, & \mbox{if } i = j \\ \mbox{0,} & \mbox{otherwise} \end{cases}.
    \label{eq:degreematrixdefinition}
\end{equation}
We can now define the graph Laplacian $L= D-A $ for the graph $\mathcal{G}$. $L$ can be normalized according to 
\begin{equation}
    L_\text{norm} = D^{-1/2} L D^{-1/2} =  I_N - D^{-1/2} A D^{-1/2},
    \label{eq:normalized_graph_laplacian}
\end{equation}
where $I_N$ is the $N$x$N$ identity matrix. In the following we assume that the graph Laplacian $L$ is normalized according to equation \eqref{eq:normalized_graph_laplacian}.

Having defined the graph laplacian $L$, we may now turn to convolutions. The convolution of the signal $x$ with a filter $g_\theta = diag(\theta)$, parametrized by a parameter $\theta \in R^N$ in the Fourier domain, can be written as 

\begin{equation}
    g_\theta * x = U g_\theta U^T x,
    \label{eq:graph_convolution}
\end{equation}
where $U$ is an orthogonal matrix containing the eigenvectors of $L$. However, performing convolutions using equation \ref{eq:graph_convolution} may be computationally intractable in practice, partly because multiplication with $U$ is $\mathcal{O}(N^2)$ and partly because calculating $U$ requires the eigendecomposition of $L$, which may be very computationally expensive for large graphs \cite{kipf_semi_supervised}. A way around this problem is discussed in Kipf et. al \cite{kipf_semi_supervised}, in which the filter $g_\theta$ in equation \ref{eq:graph_convolution} is approximated to $g_{\theta'}$ using an expansion in Chebyshev polynomials $T_k(x)$:s. To the $K^{\text{th}}$ order, the convolution $g_\theta * x$ may be written as 

\begin{equation}
    g_{\theta'} * x \approx \sum_{k=0}^K \theta'_k T_k(\tilde{L})x,
    \label{eq:convolution_approximation}
\end{equation}
where $\tilde{L} =  \frac{2}{\lambda_{\text{max}}}L - I_N$, and $\lambda_{\text{max}}$ is the largest eigenvalue of $L$. $\theta'_k$ is here a vector of Chebyshev coefficients, which in the context of machine learning corresponds to trainable parameters. The Chebyshev polynomials are recursively defined as 
\begin{equation}
    T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x), \hspace{10px} T_0(x) = 1, \hspace{5px} T_1(x) = x.
\end{equation}

The interpretation of equation \eqref{eq:convolution_approximation} is that the convolution is $K^{\text{th}}$ order local, i.e. that a convolution considers each node and their $K^{\text{th}}$ order neighbors, i.e. the neighbours that are at most $K$ steps away in the graph. 


\subsection{Layer-wise propagation rule}

Following Kipf et. al \cite{kipf_semi_supervised}, we set $K=1$ in equation \eqref{eq:convolution_approximation}, limiting the convolution to only consider a node ($k=0$) and its closest neighbours ($k=1$). This results in 

\begin{equation}
    g_{\theta'} * x \approx \theta_0' x + \theta_1' \left( \frac{2}{\lambda_{\text{max}}}L - I_N \right)x.
    \label{eq:k1_approximation_step1}
\end{equation}
We may further simplify equation \eqref{eq:k1_approximation_step1} by fixing the maximum eigenvalue of $L$, $\lambda_{\text{max}} = 2$. This is a change of scale, but one that the neural network should be able to adapt to during training. We then obtain

\begin{equation}
    g_{\theta'} * x \approx \theta_0' x + \theta_1' \left(L - I_N \right)x = \theta_0' x - \theta_1' D^{-1/2}AD^{-1/2}x ,
    \label{eq:k1_approximation_step2}
\end{equation}
and by setting a single trainable parameter $\theta = \theta_0' = -\theta_1'$ we arrive at 
\begin{equation}
    g_{\theta'} * x \approx \theta \left(I_N + D^{-1/2}AD^{-1/2} \right)x. 
    \label{eq:k1_approximation_step3}
\end{equation}
$I_N + D^{-1/2}AD^{-1/2}$ has eigenvalues in the range $[0, 2]$, and thus implementing convolutions as in equation \eqref{eq:k1_approximation_step3} in a neural network may lead to exploding or vanishing gradients. This problem can be avoided using a \textit{renormalization trick}:

\begin{equation}
    I_N + D^{-1/2}AD^{-1/2} \rightarrow \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}
    \label{eq:renormalization_trick}
\end{equation}
where $\tilde{A} = A + I_N$ and $\tilde{D} = \sum_j \tilde{A}_{ij}$.
We may now generalize this expression for a convolution operation to a signal $X \in \mathbb{R}^{N \times C}$, defined on the $N$ nodes of the graph and with a $C$-dimensional feature vector for each node. The signal $X$ thus has $C$ input channels. Furthermore, let the convolution apply $F$ filters to the $C$ input channels. The application of such a convolution can thus be written

\begin{equation}
    Z = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} X \Theta,
    \label{eq:propagation_rule}
\end{equation}
with $\Theta \in \mathbb{R}^{C\times F}$ being a matrix of filter parameters and $Z \in \mathbb{R}^{N\times F}$ as the convolved signal. This propagation rule in equation \eqref{eq:propagation_rule}, when paired with an activation function $\sigma$ such that $H = \sigma\left(Z \right)$, forms the basis for a GCN. 

Recall that we in order to arrive at this expression for a convolution set $K=1$ in the Chebyshev polynomial expansion in equation \eqref{eq:convolution_approximation}. As discussed, this limits the convolution to only consider each node and its closest neighbours. Furthermore, the approximation becomes a linear function in $L$, i.e. a function that is linear in the graph spectral domain. These two points may seem to be limitations in the representational strength of the graph convolutional layer, however this is not necessarily the case \cite{kipf_semi_supervised}. First, larger neighbourhoods can be convolved over by stacking multiple layers, with the first layer considering a node and its neghbours, the second layer considering the neighbours neighbours and so on. Secondly, keeping the representation linear in $L$ actually makes it more flexible, since it is not dependant on the explicit parametrization given by the Chebyshev polynomials. Combined, a deep GCN consisting of several stacked graph convolutional layers paired with possibly non-linear activation functions $\sigma$ can still model a rich class of convolutional filter functions, whilst keeping computational costs low. 


\subsection{Message passing as an approximation of spectral theory}
\label{subsec:message_passing}

As discussed in the previous section, a graph convolutional layer aggregates the features $x_v$ for each node $v$ and $x_u$ for all of its neighbours $u \in N(v)$, with larger neighbourhoods being considered by stacking multiple layers. Heuristically, a graph convolutional layer can thus be seen to perform a sort of \textit{message passing}, in that each node receives messages (aggregates features) from all of its closest neighbours. This can be seen by studying the propagation rule in equation \eqref{eq:propagation_rule} for a single node $i$ and a single filter channel $F=1$, and explicitly rewriting it as a sum: 

\begin{equation}
    z_i = \sum_k \theta_k \left(\hat{A}_{ii} x_{ik} + \sum_{j \neq i} \hat{A}_{ij} x_{jk} \right)
\end{equation}
where $z_i$ is the activation for node $i$, $x_{ik}$ corresponds to feature $k$ for node $i$, $\theta_k$ is the trainable weight for feature $k$ and $\hat{A} = \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$. $\hat{A}_{ij}$ is only non-zero for the nodes $j$ which are directly connected to node $i$, and thus only the nodes that are first-order neighbours to node $i$ will contribute to the sum $\sum_{j \neq i} \hat{A}_{ij} x_{jk}$. $\hat{A}_{ii}$ is a self-connection, since the $i$:th node is a zero-order neighbour to itself.
By interpreting the features $x_{ik}$ as a form of \textit{message}, the expression $\hat{A}_{ii} x_{ik} + \sum_{j \neq i} \hat{A}_{ij} x_{jk}$ can be seen as an aggregation of messages between a node's current state $x_{ik}$ and those of its neighbours $x_{jk}\left.\right\rvert_{j\neq i}$. Furthermore, in a weighted graph $\hat{A}_{ij}$ may be interpreted as the connection strength between nodes, and thus the contribution of each node can be seen as weighted by that strength. Finally, these contributions are summed up for all features $k$ with their corresponding trainable weight $\theta_k$ to yield the node's activation $z_i$. In this manner information can flow through the graph, and by stacking multiple graph convolutional layers the information can spread over successively larger neighbourhoods. See figure \ref{fig:message_passing} for a visual representation of how the ``messages'' are passed to node $i$ from it's neighbours.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{chapters/images_theory/message_passing.png}
    \caption{A schematic representation of the message passing interpretation of a graph convolutional layer. The messages $x_{jk}$ are weighted by the connection strength $\hat{A}_{ij}$ when sent from node $j$ to node $i$. All contribtions from node $i$'s neighbours are aggregated in this manner, and then multiplied by the weight $\theta_k$.}
    \label{fig:message_passing}
\end{figure}


%  The term $A_{ii} x_{ik}$ corresponds to the current state of node $i$:s, which is aggregated with the contributions from the sum $\sum_{j \neq i} A_{ij} x_{jk}$. 
% The idea behind GCNs is to aggregate the features $x_v$ for each node $v$ and $x_u$ for all of its neighbours $u \in N(v)$, and thus successively share information across the graph. Heuristically, this can be seen as a form of \textit{message passing}, in which each node communicates its features to all of its neighbours. 

\section{Machine learning on graphs}
\label{sec:ml_on_graphs}

Graphs are very versatile in storing data, and are abundant in many areas of science. It is thus interesting how one incorporates them into the framework of machine learning. Specifically, how does one formulate a problem to suitably be solved using machine learning on graphs? 

%% TODO TIKZ
% \begin{center}
%     \resizebox {\textwidth} {!} {
%         \input{chapters/images_theory/node_classification.tikz}
%     }
% \end{center}

\begin{figure}[H]
    \centering
        \begin{subfigure}{.5\textwidth}
            \centering
            \includegraphics[width=.9\linewidth]{chapters/images_theory/graph_classification.png}
            \caption{Graph classification}
            \label{fig:graph_classification}
        \end{subfigure}%
        \begin{subfigure}{.5\textwidth}
            \centering
            \includegraphics[width=.9\linewidth]{chapters/images_theory/node_classification.png}
            \caption{Node classification}
            \label{fig:node_classification}
        \end{subfigure}
    \caption{Schematic representations of node and graph classification. }
    \label{fig:graph_and_node_class}
\end{figure}

One common class of problems is \textit{graph classification}. As the name implies, given a graph $\mathcal{G}$ the task is to classify it as belonging to one of several classes. Another class of problem is \textit{node classification}. The task here is to classify each node in the set of nodes $\mathcal{V}$ belonging to the graph $\mathcal{G} = \left(\mathcal{V}, \mathcal{E} \right)$. The schematics of graph classification and node classification can be seen in figures \ref{fig:graph_classification} and \ref{fig:node_classification} respectively. Graph and node classification can be generalised to graph and node \textit{regression}, in which a continuous value is predicted for each graph or node, respectively. We will in the remainder of the thesis refer to graph and node classification/regression as graph and node prediction tasks.

In the context of neuroscience, graph prediction can be important when you have one graph per subject. These subject-specific graphs could for instance represent the brain structure of the subject. The task is then to predict some property for each subject given their graph. Node prediction could be important both on a subject level and on a population level. For subject level graphs, one could infer node level properties, such as the properties of a part of a subject's brain. On a population level, one may represent the entire population as a single \textit{population graph}, in which each node corresponds to an individual. Common machine learning tasks on population graphs includes predicting properties for each subject in the graph. In neuroscience, these properties could be the age or sex of each subject. 

In a population graph, the set of nodes $\mathcal{V}$ thus corresponds to the population, and the edges $\mathcal{E}$ relate all individuals to each other. The idea behind this construction is that the nodes contain features related to each subject, whilst the edges contain information on how the subjects relate to one another \cite{stankeviciute}. The set of edges $\mathcal{E}$ can be calculated from a similarity measure, which thus encodes information on the similarity between subjects. The similarity measure greatly shapes the resulting population graph, and should be chosen with the specific application in mind. Generally, a similarity measure $\sigma$ is often based on some inverse of a distance metric $D$. For instance, a general similarity measure between two data points $x_1$ and $x_2$ with a distance metric $D$ inverted by a kernel $K$ may be written as

\begin{equation}
    \sigma(x_1,x_2, l) = K \left(\frac{D\left(x_1, x_2\right)}{l} \right),
    \label{eq:similarity_measure}
\end{equation}
where $l$ is a hyper parameter for scaling the distance metric. With this construction, data points that are close will obtain a large similarity, and vice versa. In the context of population graphs, and assuming that subjects that are similar have similar data, similar subjects will yield a large similarity measure and thus a strong edge weight in the graph. 


\section{Analysis}
% \subsection{Model analysis/evaluation}
\subsection{Zorro}\label{sec:zorro}

The Zorro algorithm is an algorithm for determining which nodes and features are important for a model performing semi-supervised node classification. This means that the dataset consists of one big graph with one adjacency matrix $A$ and one feature matrix $X$.

%This algorithm is by the writers introduced for analysing models performing semi-supervised node classification. This means that the dataset consists of one big graph with one adjacency matrix and one feature matrix. Since we are doing graphs classification where each subject has a unique adjacency matrix some alterations must be done to the algorithm. First we, in a big picture, describe how the Zorro algorithm works and then what alterations have been made by us.

To describe the Zorro algorithm a model $\Phi_n(X,A)$ is introduced which takes as input $A$ and $X$, and gives a prediction for each node $n$ in the graph. The general idea is to replace the feature matrix $X$ by noise and then reintroduce nodes and features which makes the model prediction similar to the original. The way the noise is introduced is by 
\begin{equation*}
    Y_s = X \odot S + Z \odot (1- S), \quad Z \sim \mathcal{N}, 
\end{equation*}
where $S = \{V, F\}$ is refereed to as an explanation where the nodes $V$ and features $F$ is unmasked. The noise $\mathcal{N}$ is proposed by the authors of \cite{} to be set to the distribution of the data set. The prediction of the model $\Phi$ on the masked data is then $\Phi_n(Y_s, A)$. To evaluate an explanation $S$ i.e. if a node and feature is important for the prediction of a specific node the fidelity of the explanation is calculated by
\begin{equation}\label{eq:Ys}
    \mathcal{F}(S) = \mathbb{E}_{Y_s|Z\sim\mathcal{N}}[1_{\Phi_n(X,A) = \Phi_n(Y_s,A)}].
\end{equation}
The fidelity of an explanation is a measure of how likely it is for a model with a masked dataset to give the same prediction as a model with the original dataset.


The final algorithm then starts, for node $n$, with an empty explanation $S_n = \{\emptyset, \emptyset\}$, and thus $\Phi_n(Y_s, A)$ will have a completely random input according to equation \ref{eq:Ys}. Then the node or feature that increases the fidelity of the explanation the most is added to the explanation. More and more nodes and features are iterativelly added until the fidelity of the explanation gets higher then a hyper parameter $\tau$. Then the algorithm is finished and the resulting explanation indicates which nodes and features where important to make the prediction of specific node in the graph. This can then be repeated for all the nodes in the graph.


% \subsection{Saliency mappings etc.}


\section{Similar works: the intersection of GCNs and neuroscience}

% Ta upp relaterade arbeten och knyt ihop med introduction inför metoden. E.g. Jansson sandström, stankeviciute.

There have been numerous papers published in the last few years applying graph neural networks in neuroscience, both GCNs and other kinds of GNNs. One example of such a paper is the Master Thesis by Jansson and Sandström \cite{jansson_sandstrom}, in which the applications of GCNs in neuroscience where explored and applied on the problem of classifying Alzheimer's Disease based on MRI data. The thesis by Jansson and Sandström is actually the precursor to this very thesis, and it's their interesting findings that motivated this master thesis project. 

There have also been a fair few papers focusing on similar problems as the aim of our thesis; to develop and analyze accurate classifiers for sex and brain age prediction. For sex prediction one can note Arslan et al. \cite{arslan}, in which GCNs are applied on fMRI-data yielding a classification accuracy of $88.06\pm1.57\%$. Another example is the work by Kim and Ye \cite{understanding_gnn}, which uses a variant of GNN known as a Graph Isomorphism Network (GIN), to yield similar classification accuracies of $84.61\pm2.9\%$. When it comes to age prediction, notable works include Stankevicuete et al. \cite{stankeviciute} and Amoroso et al. \cite{multiplex}. Stankevicuete et al. applies GCNs to a population graph of subjects, where the node features consists of the MRI data for each subject. With this approach, they obtain a mean absolute error (MAE) of $28.045\pm0.595$ years and a Pearson's correlation coefficient of $r=0.675\pm0.008$. Amoroso et al. uses fully connected artificial neural networks on MRI data to predict age, and reaches a MAE of $4.7\pm0.1$ and a correlation of $r=0.95\pm0.02$.

A fair few of these related works also try to tackle the second part of the aim, the analyzation of the models to gain insight into the functioning of the human brain. For instance, Arslan et al. and Kim and Ye studies the gradients inside the neural network, using which they can estimate the importance of each node in the fMRI data. This approach is thus centered around opening up the neural network to understand it's inner workings, which makes the techniques inherently specific to GCNs and similar network types. 

What sets our thesis apart from these other works are twofold: Firstly, we develop models for both sex and age prediction that share common architectures, which simplifies comparison between the two tasks. Secondly, we analyze these models using black box methods to find functional brain networks that are linked to sex and age. The use of black box methods enables us to perform the analysis for models other than GCNs, validating the results over different methods and models.

One thing to keep in mind when comparing studies is that most often they are based on different data, which makes direct comparison of results tricky. Interestingly in our case, Arslan et al. and Stankevicuete et al. actually uses data from the same source, the UK Biobank \cite{ukbiobank},  that we use, but with some slight differences. Arslan et al. uses more highly resolved fMRI data, and Stankevicuete et al. focuses on MRI data. Even though this does not enable direct comparison, the data is a bit more similar and hence one could perhaps draw some conclusions on possible differences in results. 

